{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Artificial Neural Networks\n\nAs we have seen, Perceptrons are only capable of solving *linearly separable* problems.\nTo overcome this limitation we can connect Perceptrons together into a network, first proposed by Rumelhart. Mclelland \u0026 Hinton (1980\u0027s).\nEach one becomes a *Node* in the network and they are connected together into *Layers*.\nIn standard Artificial Neural Network (ANN) architecture there is one *input*, one *output* and one or more *hidden* layers.\nThough *input* layer is a bit misleading, it doesn\u0027t actually do any computation, it is just the inputs to the network.\n\n![ANN](resources/ANN.png \"ANN Image\")\n\nSo outputs of hidden layers become the inputs to subsequent hidden layers, or the output layer if it is the last hidden layer.\nHidden nodes tend to learn different aspects of the problem space,\nbuilding more complex decision boundaries and are therefore able to solve more complex problems.\nTo give you an intuition from the XOR problem we just saw, take a look at the following diagram.\nThe hidden nodes both learn different logical functions (AND and OR), if you combine these you have solved XOR!\n\n![ANN-XOR](resources/ANN-XOR.png \"ANN-XOR Image\")\n\n**Note:** The number of nodes in the input layer *must* equal the number of inputs/features in the data.\nThe number of output nodes *must* equal the number of labels/classes in the data.\nThe number of hidden layers and nodes in the layers is arbitrary,\nand selecting this architecture is part of building an ANN.\n\n### Differences Between Perceptrons and ANN\n\nBefore we look at the algorithm for ANN we need to understand two key differences.\n\n#### 1. Activation Function\n\nWe need each node to output a *real number*,\nso the step function we used before (which outputs 0 or 1) will not work.\nInstead we use the Sigmoid function, which \u0027squashes\u0027 the output into a real number between 0 and 1.\n\n**Note:** Other activation functions are also used, such as Tahn and ReLu, but we will stick to Sigmoid.\n\n![Activation-Functions](resources/Activation-Functions.png \"Activation-Functions Image\")\n\nWe need real numbers because:\n1. For output nodes, real numbers between 0 and 1 can be considered a *probability* of an input example belonging to a particular class.\n2. Hidden layer nodes need to produce *some* output, even if it is very small,\nso that we can calculate the error and update weights using Backpropagation.\n\n#### 2. Backpropagation and Gradient Descent\n\nWith a Perceptron we only have one layer, so,\nfrom its output we can calculate the error it produces and use that to update the weight values.\nBut now we have multiple layers what should the hidden nodes output be?\nWhat is the error and how much should we change the weights?\n\nInstead, we *share out the error* from the output nodes to the hidden nodes,\nand we do this in *proportion to the strength of the \u0027signal\u0027* (output) that it produced - hence why we need *some* output.\nSo we are *propagating* the error from the output nodes back up the network.\nThis is achieved by calculating the derivative of the error from the previous layer with respect to the weights (derivative of the error function).\nWe then use a similar weight update function that we did with Perceptrons:\n\nerror(derivative of error function) X input X learning rate\n\nWhy do we calculate the derivative of the error function? This is an algorithm called **Stochastic Gradient Descent**.\nWe want to *minimise* the error produced by a weight.\nBy calculating the derivative we get the *gradient* or the \u0027steepness\u0027 of a curve at that point (weight value).\nThe larger the gradient the further we are from the minimum error (0 gradient).\nAgain, the learning rate is how large a step we want to take towards the minimum error.\n\n![Gradient-Descent](resources/Gradient-Descent.png \"Gradient-Descent Image\")\n\n### ANN Algorithm\n\nSimilar to Perceptrons, ANN are trained in two \u0027phases\u0027.\nThe forward pass, where data is input into the network to produce an output.\nThe backward pass, where the error in output is used to update the weights using Backpropagation and Gradient Descent.\n```\n1. Set weights to random values in range [-0.5, 0.5]\n\n2. Set learning rate to a small value, usually less than 0.5\n\n3. For each training example in the dataset i.e one \u0027epoch\u0027\n\n    Forward Propagation\n    \n    For each node in the layer and each layer in turn:\n        \n        A. Sum inputs multiplied by weights\n    \n        B. Calculate Sigmoid (activation) of the sum\n    \n    Backpropagation\n    \n        C. For each node in the layer and each layer in turn **going backwards**:\n        \n        Calculate the error and derivative, first the output layer then hidden.\n        \n            output error \u003d expected/desired output - activation\n    \n            output derivative \u003d output error X derivative of sigmoid for layers output\n    \n            hidden layer error \u003d output derivative X output weights\n        \n            hidden layer derivative \u003d hidden layer error X derivative of sigmoid for layers output\n        \n        D. Update all the weights **at the same time**:\n    \n        change in weight \u003d layer derivative X input X learning rate\n    \n4. Repeat from step 3 until error is as small as possible, or (more likely) for the number of training epochs.\n```\n\nOr if you prefer maths...\n\nForward Propagation:\n\n$ a \u003d \\sigma ( \\sum\\limits_{i\u003d0}^{n} w_i \\times x_i)$\n\nBackpropagation:\n\n$ w^{\\prime} \u003d w - \\alpha\\frac{\\partial E}{\\partial w}$\n\n### Iris Dataset\n\nThe Iris Flower data set contains 150 examples from three species Iris Setosa, Iris Virginica and Iris Versicolor.\nThere are 50 examples of each species and each example has four measurements (features) Sepal Length, Sepal Width, Petal Length and Petal Width.\nThe Iris data is often used as an example for machine learning classifiers and we are going to build and test an ANN to classify the data\ni.e. given a set of measurements, what is the species?\n\n![Iris-image](resources/Iris-image.png \"Iris-image Image\")\n\nReal data very rarely comes in a format that is suitable for input to a machine learning algorithm.\nSo first we need to prepare the data ready for classification.\nIt is also often useful to visualise the data because this might help us select what kind of classifier is suitable and predict how well they might perform.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n# Read data from csv\niris \u003d pd.read_csv(\"data/Iris.csv\")\nprint(iris.head())\n\n# Plot the various combinations of 2D graph\ng \u003d sns.pairplot(iris.drop(\"Id\", axis\u003d1), hue\u003d\"Species\")",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Next we need to replace the species labels with numbers and convert them to numbers.\nIn this case we are going to use ‘one-hot encoding’,\nwhich means each species label will be replaced with a set of binary values which indicate which of the three species it is\ni.e \u0027Iris-setosa\u0027 \u003d 1 0 0, \u0027Iris-versicolor\u0027 \u003d 0 1 0 and \u0027Iris-virginica\u0027 \u003d 0 0 1.\n\nWe also need to get all of the features from the relevant columns and split the data into training and test sets.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# Replace the species with 0, 1 or 2 as appropriate\niris[\u0027Species\u0027].replace([\u0027Iris-setosa\u0027, \u0027Iris-virginica\u0027, \u0027Iris-versicolor\u0027], [0, 1, 2], inplace\u003dTrue)\n\n# Get labels, flatten and encode to one-hot\ncolumns \u003d [\u0027Species\u0027]\nlabels \u003d pd.DataFrame(iris, columns\u003dcolumns).to_numpy()\nlabels \u003d labels.flatten()\nlabels \u003d np.eye(np.max(labels) + 1)[labels]\n\n# Get Features\ncolumns \u003d [\u0027SepalLength\u0027, \u0027SepalWidth\u0027, \u0027PetalLength\u0027, \u0027PetalWidth\u0027]\nfeatures \u003d pd.DataFrame(iris, columns\u003dcolumns).to_numpy()\n\n# Split data to training and test data, 2/3 for training and 1/3 for testing\ntrain_x, test_x, train_y, test_y \u003d train_test_split(features, labels, test_size\u003d0.33)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Build the ANN\nFirst we will define the Sigmoid activation function and its derivative.\nThen, as before, we will define a few parameters such as learning rate and number of training epochs.\n\nThe main difference between our Perceptron implementation is that we will use matrices to represent our layer weights.\nIf we had to manually add new variables for each weight/node it would be quite unmanageable.\nFor example with 4 inputs and 6 hidden nodes (+ 6 bias) \u003d 30 weight variables for just one layer.\nInstead we can represent the entire layer as a matrix, in this case the hidden layer will be a 4x6 matrix.\nThis also allows us to perform the calculations on the entire layer at once, rather than using loops.\n\nThe rest of the code is the implementation of the forward and backward passes through the network.\nEvery 100 epochs we will record the mean squared error and accuracy of predictions.\nYou should see the error drop and accuracy increase smoothly(ish) over time.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# Sigmoid and its derivative\ndef sigmoid(x):\n    # This is not strictly sigmoid, but more stable when handling matrices\n    return .5 * (1 + np.tanh(.5 * x))\n\ndef sigmoid_deriv(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\n# Learning rate\nlearning_rate \u003d 0.005\n\n# Number of training epochs\nnum_epochs \u003d 2000\n\n# Network architecture parameters\nnum_features \u003d len(train_x[0])\nnum_classes \u003d len(train_y[0])\nnum_hidden_nodes \u003d 6\n\n# Initialise weights in the range -1 to 1\nnp.random.seed(1)\n# Hidden layer weights with shape \u003d number of input features x number of hidden nodes\nhidden_weights \u003d np.random.uniform(-1, 1, size\u003d(num_features, num_hidden_nodes))\nhidden_bias \u003d np.random.uniform(-1, 1, size\u003d(1, num_hidden_nodes))\n# Output layer weights with shape \u003d number of hidden nodes x number of output classes\noutput_weights \u003d np.random.uniform(-1, 1, size\u003d(num_hidden_nodes, num_classes))\noutput_bias \u003d np.random.uniform(-1, 1, size\u003d(1, num_classes))\n\n# For recording error and accuracy - for graph later\ntraining_errors \u003d []\ntesting_errors \u003d []\ntraining_accuracies \u003d []\ntesting_accuracies \u003d []\n\n# Train for number of epochs\nfor epoch in range(num_epochs):\n\n    # Forward pass\n    input_layer \u003d train_x\n    # sigmoid( (W * X) + b)\n    hidden_layer \u003d sigmoid(np.dot(input_layer, hidden_weights) + hidden_bias)\n    output_layer \u003d sigmoid(np.dot(hidden_layer, output_weights) + output_bias)\n\n    # Backpropagation using gradient descent\n    # Calculate output layer error\n    output_layer_error \u003d train_y - output_layer\n    # Calculate output layer derivative Note: that we just need this layers error for the bias\n    output_layer_delta \u003d output_layer_error * sigmoid_deriv(output_layer)\n    output_bias_delta \u003d np.sum(output_layer_error, axis\u003d0)\n    \n    # Calculate hidden layer error (from the output layers weights and derivative\n    hidden_layer_error \u003d output_layer_delta.dot(output_weights.T)\n    # Calculate hidden layer derivative Note: that we just need this layers error for the bias\n    hidden_layer_delta \u003d hidden_layer_error * sigmoid_deriv(hidden_layer)\n    hidden_bias_delta \u003d np.sum(hidden_layer_error, axis\u003d0)\n\n    # Update the weights (learning rate X layers input X layers derivative)\n    output_weights +\u003d learning_rate * hidden_layer.T.dot(output_layer_delta)\n    output_bias +\u003d learning_rate * output_bias_delta\n    \n    hidden_weights +\u003d learning_rate * input_layer.T.dot(hidden_layer_delta)\n    hidden_bias +\u003d learning_rate * hidden_bias_delta\n\n    # Every 100 epochs record error and accuracy during training\n    if (epoch % 100) \u003d\u003d 0:\n        \n        # Mean squared error over all errors this epoch\n        error \u003d 0.5 * np.mean(np.abs(output_layer_error)) ** 2\n        training_errors.append(error)\n\n        accuracy_count \u003d 0\n        for i in range(len(output_layer)):\n          \n            # Get the prediction i.e. the output with the highest value\n            prediction \u003d np.argmax(output_layer[i])\n            # Get the actual label\n            actual_label \u003d np.argmax(train_y[i])\n            \n            # If they match the prediction was correct\n            if prediction \u003d\u003d actual_label:\n                accuracy_count +\u003d 1\n        accuracy \u003d (len(train_x) / 100) * accuracy_count\n        training_accuracies.append(accuracy)\n        \n        ##YOUR CODE STARTS HERE##     \n\n        ##YOUR CODE ENDS HERE##\n        print(\"Epoch: \" + str(epoch) +\n              \" Error: \" + str(round(error, 5)) +\n              \" Accuracy: \" + str(accuracy) + \"%\")\n        \n# Plot the error chart\nplt.plot(training_errors)\nplt.plot(testing_errors)\nplt.xlabel(\u0027Epochs\u0027)\nplt.ylabel(\u0027Error\u0027)\nplt.show()\n\n# Plot the accuracy chart\nplt.plot(training_accuracies)\nplt.plot(testing_accuracies)\nplt.xlabel(\u0027Epochs\u0027)\nplt.ylabel(\u0027Accuracy\u0027)\nplt.show()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Testing the Network\nYou should see quite a high accuracy and low error on the training set.\nThough in reality this is not a good measure of how well the network will perform\nbecause it has already \u0027seen\u0027 the training data and used that to adjust the weights.\nHow well would it perform on data that it has not seen? How well will it \u0027generalise\u0027 to new data?\n\nThis is why we kept 1/3rd of our data for testing!\n\nLets apply our learned model to the test set and see how well it is able to classify flowers that it has not seen before.\nYou can either do this in a new cell below,\nor (recommended) add code above in the section where we are measuring error and accuracy every 100 epochs.\nThat way we can track progress as the network trains.\nYou can even add the testing errors and accuracies to the plots for comparison.\n\nSo you need to:\n1. Perform a forward pass **only**, no updating weights, using the test inputs.\n2. Calculate the error on the test labels after the forward pass.\n3. Calculate the accuracy on the test set.\n\n**Note:** It is recommended that you create new variables for everything **except** the weight/bias matrices.\nThose matrices are the model we have trained.\n\nYou can also try different learning rates/epochs or anything else to see what effect it has.\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}