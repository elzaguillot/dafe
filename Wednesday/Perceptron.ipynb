{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Perceptrons - The basis of Artificial Neural Networks\n\nPerceptrons, invented by Frank Rosenblatt in the late 1950\u0027s,\nare a form of supervised machine learning algorithm inspired by neuron cells.\nIn neurons, signals come in along the dendrites and out along the axon. \nA synapse is the connection between the axon of one cell and the dendrites of another.\nCrudely, input signals are \u0027summed\u0027 and if they reach a certain threshold the neuron \u0027fires\u0027\nand sends a signal down the synapse to the connected cells.\n\n![Perceptron](resources/Perceptron.png \"Perceptron Image\")\n\nPerceptrons are an algorithmic approximation of this process and can learn to solve simple classification problems.\nInput values are multiplied by a learnable parameter called a *weight*.\nIf the sum of the inputs X weights is over a certain threshold the Perceptron \u0027fires\u0027 and generates an output.\nWe use the *error* in the output to change the value of the *weights* by a small amount - the *learning rate*.\nThe process is repeated until the error is 0, or as small as we can get it.\n\n**Note:** The threshold which determines if the Perceptron produces an output is determined by its *activation function*.\nFor Perceptrons this is usually a step function which outputs a 1 or 0 i.e. \u0027fires\u0027 or not.\n\n### Perceptron - Algorithm\n```\n1. Set weights to random values in range [-0.5, 0.5]\n\n2. Set learning rate to a small value, usually less than 0.5\n\n3. For each training example in the dataset i.e one \u0027epoch\u0027\n\n    A. Calculate output (activation)\n    \n    Sum inputs multiplied by weights\n    \n    If sum is greater than 0, output \u003d 1, otherwise output \u003d 0\n    \n    B. Calculate error\n    \n    error \u003d expected/desired output - activation\n\n    C. Update each of the weights values\n    \n    change in weight \u003d error X input X learning rate\n    \n4. Repeat from step 3 until error is 0 (or as close as possible), or for the number of training epochs.\n```\nOr if you prefer maths...\n\n$ y \u003d 1\\ if\\ \\sum\\limits_{i\u003d0}^{n} w_i \\times x_i \\geq\\ 0 \\\\else \\\\ y \u003d 0\\ if\\ \\sum\\limits_{i\u003d0}^{n} w_i \\times x_i \\lt\\ 0 \\\\where \\ x_0 \u003d 1 \\ and\\ w_0 \u003d -\\theta $\n\n**Note:** The last line here is for the bias, where the input is always 1.\nWeights are usually denoted by $\\theta$ (theta) and the bias weight tends towards the negative.\n\n### Perceptrons - Logical Operators\n\nLogical operators, also know as logical functions or boolean functions,\noriginate from propositional logic and form the basis logic gates for computation.\n\nThere are only two possible values, True and False, represented as 1 and 0.\nThe functions can be represented using truth tables, with two inputs and one output.\nBelow is the truth table for the \u0027AND\u0027 function:\n\n| Input 1| Input 2| AND |\n|:------:|:------:|:---:|\n| 0      | 0      | 0   |\n| 0      | 1      | 0   |\n| 1      | 0      | 0   |\n| 1      | 1      | 1   |\n\nWe are going to use this data to show how Perceptrons can learn to represent these logical functions,\nthough you could also think about it as a prediction/classification problem\ni.e. for a given set of inputs what is the correct output.\nThis table is therefore the Perceptrons *training* data, with each row representing an input example.\nEach training example has two inputs (*features*) and one output (*label*).\n\nYou can also plot these functions on a graph, which will be handy later, so first lets define our data and make a plot.\n\n**Note:** Input data is often denoted by X and labels with Y,\nso here we are going to use train_x and train_y for our variable names.\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Training data\ntrain_x \u003d np.array([[0, 0],\n                   [0, 1],\n                   [1, 0],\n                   [1, 1]])\n\ntrain_y \u003d np.array([0, 0, 0, 1]) # AND\n\n# Plot each point on a graph, \u0027o\u0027 for true and \u0027x\u0027 for false\nfor i in range(len(train_y)):\n    if train_y[i] \u003d\u003d 1:\n        plt.scatter(train_x[i, 0], train_x[i, 1], marker\u003du\u0027o\u0027, facecolors\u003d\u0027green\u0027)\n    else:\n        plt.scatter(train_x[i, 0], train_x[i, 1], marker\u003du\u0027x\u0027, facecolors\u003d\u0027red\u0027)\nplt.title(\"Logical AND\")\nplt.show()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Perceptron - Implementation\n\nNow lets write a function to build and train a Perceptron for the AND function.\nThis is just an implementation of the algorithm above.\nAs it trains you should see output for the current training epoch and total error for that epoch.\nThe error should quickly reach 0, if it doesn\u0027t try increasing the number of epochs or changing the learning rate.\n\nThe num_epochs variable determines how many times we will show the Perceptron *all* of the training data.\n\nThe learning_rate variable determines how large a change we will make to the weights each time they are updated.\nThe learning rate is often denoted as $\\alpha$ (alpha).",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "num_epochs \u003d 10\nlearning_rate \u003d 0.5\n\ndef train_perceptron(inputs, desired_outputs, training_epochs, alpha):\n    \n    # Set the weights to small random values in the range -0.5 to 0.5\n    bias \u003d 1\n    bias_w \u003d np.random.uniform(-0.5, 0.5)\n    w1 \u003d np.random.uniform(-0.5, 0.5)\n    w2 \u003d np.random.uniform(-0.5, 0.5)\n    \n    # Each epoch will loop over the training data once\n    for epoch in range(training_epochs):\n        \n        total_error \u003d 0\n        # Loop over all of the input examples\n        for i in range(len(inputs)):\n            \n            # Calculate output\n            weight_sum \u003d (inputs[i][0] * w1) + (inputs[i][1] * w2) + (bias * bias_w)\n            \n            if weight_sum \u003e 0: # Activation (step) function\n                activation \u003d 1\n            else:\n                activation \u003d 0\n    \n            # Calculate error (desired output - actual output)\n            error \u003d desired_outputs[i] - activation\n            total_error +\u003d np.absolute(error) # Also keep track of total error for this epoch\n            \n            # Update weights (error * input * learning rate)\n            w1 +\u003d error * inputs[i][0] * alpha\n            w2 +\u003d error * inputs[i][1] * alpha\n            bias_w +\u003d error * bias * alpha\n        \n        print(\"Epoch: \" + str(epoch + 1) + \" Error: \" + str(total_error))\n    return w1, w2, bias_w\n\n# Call the function to train the Perceptron and return the trained weights\ninput_w1, input_w2, bias_w \u003d train_perceptron(train_x, train_y, num_epochs, learning_rate)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Perceptrons - Linear Decision Boundary\n\nTo give you an intuition around what the Perceptron is doing, consider the equation for a straight line:\n\ny \u003d ax + c\n\na and c are coefficients just like the learned weights and bias in the Perceptron.\nSo with a bit of rearranging:\n\ny \u003d ((input1 x weight1) + (input2 x weight2)) + bias weight\n\nBecomes:\n\ninput2 \u003d ((-weight1/weight2) * input1) + (-bias weight/weight2)\n\nweights 1 and 2 \u003d slope\n\nbias \u003d intercept\n\nstep function \u003d which side of the line!\n\nSo, the Perceptron is essentially learning a function for a straight line which is called the decision boundary.\nIn this case, which \u0027class\u0027 the set of inputs belongs to i.e. True or False.\n\nWe can add this to our plot from before to visualise it.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# Print the learned weight values\nprint(\"Input weight 1 \u003d \" + str(input_w1))\nprint(\"Input weight 2 \u003d \" + str(input_w2))\nprint(\"Bias weight \u003d \" + str(bias_w))\n\nx \u003d np.linspace(0, 1)\ny \u003d ((-input_w1/input_w2) * x) + (-bias_w/input_w2)\n\nfor i in range(len(train_y)):\n    if train_y[i] \u003d\u003d 1:\n        plt.scatter(train_x[i, 0], train_x[i, 1], marker\u003du\u0027o\u0027, facecolors\u003d\u0027green\u0027)\n    else:\n        plt.scatter(train_x[i, 0], train_x[i, 1], marker\u003du\u0027x\u0027, facecolors\u003d\u0027red\u0027)\nplt.plot(x,y)\nplt.title(\"Logical AND Decision Boundary\")\nplt.show()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Perceptrons - OR and XOR\n\nNow lets try the Perceptron for the OR and XOR functions. The truth table for these is below.\n\n| Input 1| Input 2| OR  | XOR |\n|:------:|:------:|:---:|:---:|\n| 0      | 0      | 0   | 0   |\n| 0      | 1      | 1   | 1   |\n| 1      | 0      | 1   | 1   |\n| 1      | 1      | 1   | 0   |\n\nRemember the inputs are the same so you should only need to specify the new labels/desired outputs.\nYou can also change the number of training epochs and learning rate if you wish,\nbut stick with the values that worked for AND first.\n\nJust call the train_perceptron() function as before with the new values.\nYou can also copy the code above to display the function and its decision boundary after training",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n##YOUR CODE HERE##\ntrain_y \u003d np.array([0, 1, 1, 0]) # XOR\ninput_w1, input_w2, bias_w \u003d train_perceptron(train_x, train_y, num_epochs, learning_rate)\n\nx \u003d np.linspace(0, 1)\ny \u003d ((-input_w1/input_w2) * x) + (-bias_w/input_w2)\n\nfor i in range(len(train_y)):\n    if train_y[i] \u003d\u003d 1:\n        plt.scatter(train_x[i, 0], train_x[i, 1], marker\u003du\u0027o\u0027, facecolors\u003d\u0027green\u0027)\n    else:\n        plt.scatter(train_x[i, 0], train_x[i, 1], marker\u003du\u0027x\u0027, facecolors\u003d\u0027red\u0027)\nplt.plot(x,y)\nplt.title(\"Logical AND Decision Boundary\")\nplt.show()\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}